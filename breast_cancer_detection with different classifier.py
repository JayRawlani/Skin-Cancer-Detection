# -*- coding: utf-8 -*-
"""Breast Cancer Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1md6ULR3Z9tLpGHqOgAysaspbL7kGK0i1
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries:
import os
import numpy as np
import pandas as pd
import seaborn as sns
import datetime as dt
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

# Reading the file
data = pd.read_csv('Breast_Cancer_data.csv')

# Checking the first few rows:
data.head()

# Target Variable:
data.diagnosis.unique()

data.describe()

# Dropping some of the unwanted variables:
data.drop('id',axis=1,inplace=True)
data.drop('Unnamed: 32',axis=1,inplace=True)

# Binarizing the target variable:
data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})

datas = pd.DataFrame(preprocessing.scale(data.iloc[:,1:32]))
datas.columns = list(data.iloc[:,1:32].columns)
datas['diagnosis'] = data['diagnosis']

#Looking at the number of patients with Malignant and Benign Tumors:
datas.diagnosis.value_counts().plot(kind='bar', alpha = 0.5, facecolor = 'r', figsize=(6,6))
plt.title("Diagnosis (M=1 , B=0)", fontsize = '18')
plt.ylabel("Total Number of Patients")

data.columns

data_mean = data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]

#Visualizing dataset using heatmap
plt.figure(figsize=(14,14))
foo = sns.heatmap(data_mean.corr(), vmax=1, square=True, annot=True)

#Subplot grid for plotting pairwise relationships in a dataset
from pandas.plotting import scatter_matrix
p = sns.PairGrid(data_mean, hue="diagnosis")
p.map(sns.scatterplot)
p.map_upper(plt.scatter, s = 10, edgecolor = 'w')
p.map_diag(plt.hist)
p.map_lower(sns.kdeplot)
p.add_legend()

p.figsize = (30,30)

_ = sns.swarmplot(y='radius_mean',x='diagnosis', data=data_mean)
plt.show()

j = sns.swarmplot(y='area_mean',x='diagnosis', data=data_mean)
plt.show(j)

from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn import metrics

predictors = data_mean.columns[2:11]
target = "diagnosis"

X = data_mean.loc[:,predictors]
y = np.ravel(data.loc[:,[target]])

# Split the dataset in train and test:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""**Finding the Accuracy of training dataset**

"""

from sklearn.linear_model import LogisticRegression

# Initiating the model:
lr = LogisticRegression()

lr_train_scores = cross_val_score(lr, X_train, y_train, scoring='accuracy').mean()

print("The mean accuracy of training dataset using Logistic Regression is %s" % round(lr_train_scores*100,2))

# Importing the model:
from sklearn import svm

svm = svm.SVC()

svm_train_scores = cross_val_score(svm, X_train, y_train, scoring='accuracy').mean()

print("The mean accuracy of training dataset using SVM is %s" % round(svm_train_scores*100,2))

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

knn_train_scores = cross_val_score(knn, X_train, y_train, scoring='accuracy').mean()

print("The mean accuracy of training dataset using KNeighbours Classifier is %s" % round(knn_train_scores*100,2))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()

rf_train_scores = cross_val_score(rf, X_train, y_train, scoring='accuracy').mean()

print("The mean accuracy of training dataset RandomForest Classifier is %s" % round(rf_train_scores*100,2))

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb_train_scores = cross_val_score(rf, X_train, y_train, scoring='accuracy').mean()

print("The mean accuracy of training dataset using GuassianNB is %s" % round(nb_train_scores*100,2))

"""**Finding the Accuracy on Testing dataset**

"""

from sklearn.linear_model import LogisticRegression

# Initiating the model:
lr = LogisticRegression()

lr_test_scores = cross_val_score(lr, X_test, y_test, scoring='accuracy').mean()

print("The mean accuracy of test dataset using Logistic Regression is %s" % round(lr_test_scores*100,2))

from sklearn import svm
svm = svm.SVC()

svm_test_scores = cross_val_score(svm, X_test, y_test, scoring='accuracy').mean()

print("The mean accuracy of test dataset using SVM is %s" % round(svm_test_scores*100,2))

# Check Accuracy using KNN for different values of neighbors
for i in range(1, 21):
    knn = KNeighborsClassifier(n_neighbors = i)
    score = cross_val_score(knn, X_train, y_train, scoring='accuracy').mean()
    print("N = " + str(i) + " :: Score = " + str(round(score,2)))

from sklearn.neighbors import KNeighborsClassifier

# Finding the Accuracy on Testing dataset
knn = KNeighborsClassifier(n_neighbors = 4)

knn_test_scores = cross_val_score(knn, X_test, y_test, scoring='accuracy').mean()

print("The mean accuracy of test dataset using KNeighbours Classifier is %s" % round(knn_test_scores*100,2))

# Checking Accuracy of Random Forest using diffenent number of estimators
for i in range(1, 21):
    rf = RandomForestClassifier(n_estimators = i)
    score = cross_val_score(rf, X_train, y_train, scoring='accuracy').mean()
    print("N = " + str(i) + " :: Score = " + str(round(score,2)))

from sklearn.ensemble import RandomForestClassifier
# Finding the Accuracy on Testing dataset

rf = RandomForestClassifier(n_estimators=16)

rf = rf.fit(X_train, y_train)

predicted = rf.predict(X_test)

rf_test_score = metrics.accuracy_score(y_test, predicted)

print ('The accuracy of test data using Random Forest Classifier is %s' % (round(rf_test_score*100,2)))

from sklearn.naive_bayes import GaussianNB
# Finding the Accuracy on Testing dataset

nb = GaussianNB()

nb = nb.fit(X_train, y_train)

predicted = nb.predict(X_test)

nb_test_score = metrics.accuracy_score(y_test, predicted)

print ('The accuracy of test data using GaussianNB is %s' % (round(nb_test_score*100,2)))

df = pd.DataFrame([[lr_train_scores, lr_test_scores], [svm_train_scores, svm_test_scores], [knn_train_scores, knn_test_scores], [rf_train_scores, rf_test_score], [nb_train_scores, nb_test_score]], columns=(['Training Accuracy','Testing Accuracy']))
df

# creating dataframe
df = pd.DataFrame({
    'Name': ['Logistic Regression', 'SVM', 'KNN', 'RandomForest', 'GussianNB'],
    'Training Accuracy': [lr_train_scores, svm_train_scores, knn_train_scores, rf_train_scores, nb_train_scores],
    'Testing Accuracy': [lr_test_scores, svm_test_scores, knn_test_scores, rf_test_score, nb_test_score]
})
  
# plotting graph
df.plot(x="Name", y=["Training Accuracy", "Testing Accuracy"], kind="bar")
plt.xticks(rotation=45)